# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** praveerbn
**Total Score:** 27/40 (67.5%)

**Grade Category:** D (Poor)

---

## Problem Breakdown

### Exercise 1 (7/16 = 43.8%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good work: you fit PCA to 2 components and visualized the training set colored by labels. For a fuller solution, consider also visualizing reconstructed digits from varying numbers of components and/or projecting the test set. Plot is otherwise appropriate.

**Part pipeline-part2** (pipeline-part2.code): 2/4 points

_Feedback:_ You correctly applied PCA and produced a scree plot, showing understanding. However, the task required reducing to 2 components and visualizing a 2D scatter colored by class. No 2D transform or class-colored scatter is shown. Reduce to 2 PCs and scatter-plot by y.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You fit PCA with n_components=0.95 and printed the count, but the task was to compute and visualize a scree plot for the first 40 components with y-axis as percent variance explained. No scree plot produced and not limited to 40 PCs. Partial credit for correct PCA usage.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Full credit. You already computed the components for 95% variance using pca95.n_components_ in prior work, and here you correctly use pca95 to transform/inverse-transform a digit. This demonstrates correct application of your PCA fitted for 95% variance.

**Part pipeline-part5** (pipeline-part5.code): 0/4 points

_Feedback:_ This cell doesn’t address Step 5. You were asked to visualize a digit reconstructed from the reduced space using the dimensions from Step 4 (your pca95). Instead, you trained/evaluated KNN. Please transform one digit with pca95, inverse_transform, and plot it.

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good job: correct t-SNE usage, sensible subsampling, parameters, and colored scatter by labels. This meets the goal. For clarity, consider adding a colorbar and a discrete colormap (e.g., tab10).

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you embed data with t-SNE, train KNN on train embedding, and report test accuracy. One caveat: you fit t-SNE on the combined train+test, which leaks test info. Prefer fitting t-SNE on train only, then transform test. Otherwise solid.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job. You fit UMAP on train, transformed train/test, trained KNN on the embedding, and reported test accuracy. This correctly calculates accuracy with KNN on UMAP features. Choice of 30D is fine.

---

### Exercise 4 (10/14 = 71.4%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ Good PCA→KNN sweep (d=1–3) and 2D visualization. However, you didn’t implement UMAP at all: no dimensionality sweep, no parameter variations (e.g., n_neighbors, min_dist), no UMAP visualization, and no KNN evaluation on UMAP embeddings.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP instead of the required PCA. While your approach (fit-transform, KNN scoring across dimensions, and a 2D scatter) is sound, it doesn’t follow the PCA instruction. Re-implement using sklearn.decomposition.PCA and plot PC1/PC2 to earn full credit.

**Part ex2-part3** (ex2-part3.answer): 7/7 points

_Feedback:_ Good explanation: you correctly contrast UMAP’s neighborhood preservation with PCA’s variance focus, and explain why PCA in low dims can miss the label-informative z while UMAP performs better. Your linkage to observed accuracies and the 3-PC improvement is coherent.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:15 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*